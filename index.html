<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    /* 视频容器样式调整 */
    .video-group {
      margin-bottom: 2rem;
      text-align: center; /* 标题和视频组整体居中 */
    }
    
    .video-row {
      display: flex;
      flex-wrap: wrap;
      justify-content: center;
      gap: 1.5rem; /* 视频之间的间距 */
      margin-bottom: 1.5rem;
    }
    
    .video-item {
      width: calc(25% - 1rem); /* 上排四个视频宽度均分（若需上三下二，此处改为33.333%） */
      min-width: 160px;
      max-width: 200px;
    }

    /* 上三下二布局专用类 */
    .row-three {
      width: calc(33.333% - 1rem);
    }
    .row-two {
      width: calc(20% - 1rem); /* 下排两个视频宽度适配，居中分布 */
    }
    
    .video-container {
      position: relative;
      width: 100%;
      height: 0;
      padding-bottom: 56.25%; /* 16:9 比例，确保视频尺寸统一 */
      overflow: hidden;
      border-radius: 4px;
      background-color: #f0f0f0;
    }
    
    .video-container video {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: contain;
    }
    
    .video-caption {
      text-align: center;
      margin-top: 0.5rem;
      font-size: 0.85rem;
      color: #333;
    }
    
    /* 视频组标题样式 */
    .video-group-title {
      margin: 2rem 0 1.5rem;
      font-size: 1.2rem;
      font-weight: bold;
      color: #333;
    }
    
    /* 响应式调整 */
    @media (max-width: 768px) {
      .video-item {
        width: calc(50% - 1rem);
        max-width: 140px;
      }
      
      .row-three, .row-two {
        width: calc(50% - 1rem);
      }
      
      .video-group-title {
        font-size: 1rem;
      }
    }
  </style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Kang Xiangli</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Yage He</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Xianwu Gong<sup>*</sup></a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Zhan Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Yuru Bai</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>School of Electronics and Control Engineering, Chang'an University, Xi'an, Shaanxi, China</span><br>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block">Email: kxiangli@chd.edu.cn, heyage@chd.edu.cn, xwgong@chd.edu.cn, chdlzh@chd.edu.cn, baiyuru@chd.edu.cn</span>
            <p class="is-size-6" style="margin-top: 0.3rem; font-style: italic;">* Corresponding author</p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- 可在此处添加论文链接（如PDF、代码等），当前已移除 -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This study presents a grasping method for objects with uneven mass distribution by leveraging diffusion models to localize the center of gravity (CoG) on unknown objects. In robotic grasping, CoG deviation often leads to postural instability, where existing keypoint-based or affordance-driven methods exhibit limitations. We constructed a dataset of 790 images featuring unevenly distributed objects with keypoint annotations for CoG localization. A vision-driven framework based on foundation models was developed to achieve CoG-aware grasping. Experimental evaluations across real-world scenarios demonstrate that our method achieves a 49\% higher success rate compared to conventional keypoint-based approaches and an 11\% improvement over state-of-the-art affordance-driven methods. The system exhibits strong generalization with a 76\% CoG localization accuracy on unseen objects.This provides an innovative solution for precise and stable grasping tasks, with its scientific validity further validated in complex and dynamic scenarios.
          </p>
          
        </div>
      </div>
    </div>
    
    <!--/ Abstract. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Overview</h2>
    <div class="publication-image">
      <!-- 显示框图 -->
      <img src="static/images/框图.png" alt="系统框图" style="max-width:100%;height:auto;">
      <!-- 合并后的文字说明 -->
      <div style="margin-top:8px; font-size:0.9rem; color:#666; text-align:center;">
        <p>
          <strong>Left:</strong> Given the instruction and current RGB-D images, the VLM first identifies the target object. It then generalizes the segmented image and maps the center of gravity to the current scene using the center-of-gravity module, selecting the nearest and highest-scoring grasp pose. 
          <strong>Right:</strong> The image on the right shows the captured result in a real-world scenario, depicting the grasped objects: a rasp, crowbar, pliers, and ratchet wrench. The method described in this paper enables grasping of unseen objects without requiring additional training.
        </p>
      </div>
    </div>
  </div>
</div>
    
    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">In the real world</h2>
      <p style ="text-align: left;">
        In the real world, we conducted experiments using the Piper robotic arm, with point cloud data acquired via the D435 scanner. We compiled an image dataset comprising 790 centre-of-gravity points. The dataset includes common household objects such as hammers, hand files, ratchet wrenches, screwdrivers, and pincers. We tested ten objects with uneven mass distributions in real-world settings, evaluating the model's performance in both complex and dynamic scenarios.
      </p>
      
      <!-- 第一组视频（复杂环境） -->
      <div class="video-group">
        <div class="video-group-title">Closed-loop Grasping in Complex Environments</div>
        
        <!-- 上排三个视频 -->
        <div class="video-row">
          <div class="video-item row-three">
            <div class="video-container">
              <video controls loop>
                <source src="static/videos/扳手.mp4" type="video/mp4">
              </video>
            </div>
            <div class="video-caption">Wrench</div>
          </div>
          <div class="video-item row-three">
            <div class="video-container">
              <video controls loop>
                <source src="static/videos/锤子.mp4" type="video/mp4">
              </video>
            </div>
            <div class="video-caption">Hammer</div>
          </div>
          <div class="video-item row-three">
            <div class="video-container">
              <video controls loop>
                <source src="static/videos/搓刀.mp4" type="video/mp4">
              </video>
            </div>
            <div class="video-caption">File</div>
          </div>
        </div>
        
        <!-- 下排两个视频 -->
        <div class="video-row">
          <div class="video-item row-two">
            <div class="video-container">
              <video controls loop>
                <source src="static/videos/螺丝刀.mp4" type="video/mp4">
              </video>
            </div>
            <div class="video-caption">Screwdriver</div>
          </div>
          <div class="video-item row-two">
            <div class="video-container">
              <video controls loop>
                <source src="static/videos/钳子.mp4" type="video/mp4">
              </video>
            </div>
            <div class="video-caption">Pliers</div>
          </div>
        </div>
      </div>

      <!-- 第二组视频（动态环境） -->
      <div class="video-group">
        <div class="video-group-title">Closed-loop Grasping in Dynamic Environments</div>
        
        <!-- 上排三个视频 -->
        <div class="video-row">
          <div class="video-item row-three">
            <div class="video-container">
              <video controls loop>
                <source src="static/videos/扳手_动态.mp4" type="video/mp4">
              </video>
            </div>
            <div class="video-caption">Wrench</div>
          </div>
          <div class="video-item row-three">
            <div class="video-container">
              <video controls loop>
                <source src="static/videos/锤子_动态.mp4" type="video/mp4">
              </video>
            </div>
            <div class="video-caption">Hammer</div>
          </div>
          <div class="video-item row-three">
            <div class="video-container">
              <video controls loop>
                <source src="static/videos/搓刀_动态.mp4" type="video/mp4">
              </video>
            </div>
            <div class="video-caption">File</div>
          </div>
        </div>
        
        <!-- 下排两个视频 -->
        <div class="video-row">
          <div class="video-item row-two">
            <div class="video-container">
              <video controls loop>
                <source src="static/videos/螺丝刀_动态.mp4" type="video/mp4">
              </video>
            </div>
            <div class="video-caption">Screwdriver</div>
          </div>
          <div class="video-item row-two">
            <div class="video-container">
              <video controls loop>
                <source src="static/videos/钳子_动态.mp4" type="video/mp4">
              </video>
            </div>
            <div class="video-caption">Pliers</div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

</body>
</html>
