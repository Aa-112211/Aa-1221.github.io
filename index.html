<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Deformable Neural Radiance Fields creates free-viewpoint portraits (nerfies) from casually captured videos.">
  <meta name="keywords" content="Nerfies, D-NeRF, NeRF">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>

  <style>
    /* 统一视频容器尺寸 */
    .video-item {
      width: 100%;
      max-width: 160px; /* 可根据需求调整 */
      margin: 0 auto;
    }
    .video-container {
      position: relative;
      padding-bottom: 56.25%; /* 16:9 比例 */
      height: 0;
      overflow: hidden;
    }
    .video-container video {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      object-fit: cover;
    }
    .video-caption {
      text-align: center;
      margin-top: 0.5rem;
      font-size: 0.85rem;
    }
  </style>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>

</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Foundation Model-Driven Grasping of Unknown Objects via Center of Gravity Estimation</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#">Kang Xiangli</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#">Yage He</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="#">Xianwu Gong<sup>*</sup></a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Zhan Liu</a><sup>1</sup>,
            </span>
            <span class="author-block">
              <a href="#">Yuru Bai</a><sup>1</sup>
            </span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>School of Electronics and Control Engineering, Chang'an University, Xi'an, Shaanxi, China</span><br>
            <span class="author-block"><sup>2</sup>Chang'an University, Xi'an, Shaanxi, China</span>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block">Email: kxiangli@chd.edu.cn, heyage@chd.edu.cn, xwgong@chd.edu.cn, chdlzh@chd.edu.cn, baiyuru@chd.edu.cn</span>
            <p class="is-size-6" style="margin-top: 0.3rem; font-style: italic;">* Corresponding author</p>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- 可在此处添加论文链接（如PDF、代码等），当前已移除 -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

  
<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            This study presents a grasping method for objects with uneven mass distribution by leveraging diffusion models to localize the center of gravity (CoG) on unknown objects. In robotic grasping, CoG deviation often leads to postural instability, where existing keypoint-based or affordance-driven methods exhibit limitations. We constructed a dataset of 790 images featuring unevenly distributed objects with keypoint annotations for CoG localization. A vision-driven framework based on foundation models was developed to achieve CoG-aware grasping. Experimental evaluations across real-world scenarios demonstrate that our method achieves a 49\% higher success rate compared to conventional keypoint-based approaches and an 11\% improvement over state-of-the-art affordance-driven methods. The system exhibits strong generalization with a 76\% CoG localization accuracy on unseen objects.This provides an innovative solution for precise and stable grasping tasks, with its scientific validity further validated in complex and dynamic scenarios.
          </p>
          
        </div>
      </div>
    </div>
    
    <!--/ Abstract. -->
<div class="columns is-centered has-text-centered">
  <div class="column is-four-fifths">
    <h2 class="title is-3">Overview</h2>
    <div class="publication-image">
      <!-- 显示框图 -->
      <img src="static/images/框图.png" alt="系统框图" style="max-width:100%;height:auto;">
      <!-- 合并后的文字说明 -->
      <div style="margin-top:8px; font-size:0.9rem; color:#666; text-align:center;">
        <p>
          <strong>Left:</strong> Given the instruction and current RGB-D images, the VLM first identifies the target object. It then generalizes the segmented image and maps the center of gravity to the current scene using the center-of-gravity module, selecting the nearest and highest-scoring grasp pose. 
          <strong>Right:</strong> The image on the right shows the captured result in a real-world scenario, depicting the grasped objects: a rasp, crowbar, pliers, and ratchet wrench. The method described in this paper enables grasping of unseen objects without requiring additional training.
        </p>
      </div>
    </div>
  </div>
</div>
    
    <div class="columns is-centered has-text-centered">
    <div class="column is-four-fifths">
      <h2 class="title is-3">In the real world</h2>
      <p style ="text-align: left;">
        In the real world, we conducted experiments using the Piper robotic arm, with point cloud data acquired via the D435 scanner. We compiled an image dataset comprising 790 centre-of-gravity points. The dataset includes common household objects such as hammers, hand files, ratchet wrenches, screwdrivers, and pincers. We tested ten objects with uneven mass distributions in real-world settings, evaluating the model's performance in both complex and dynamic scenarios.
      </p>
      
      <!-- 第一行视频（复杂环境） -->
      <div class="columns is-multiline is-centered" style="margin-top: 2rem;">
        <div class="column is-one-fifth video-item">
          <div class="video-container">
            <video controls loop>
              <source src="static/videos/扳手.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-caption">复杂环境下扳手抓取</div>
        </div>
        <div class="column is-one-fifth video-item">
          <div class="video-container">
            <video controls loop>
              <source src="static/videos/锤子.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-caption">复杂环境下锤子抓取</div>
        </div>
        <div class="column is-one-fifth video-item">
          <div class="video-container">
            <video controls loop>
              <source src="static/videos/搓刀.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-caption">复杂环境下搓刀抓取</div>
        </div>
        <div class="column is-one-fifth video-item">
          <div class="video-container">
            <video controls loop>
              <source src="static/videos/螺丝刀.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-caption">复杂环境下螺丝刀抓取</div>
        </div>
        <div class="column is-one-fifth video-item">
          <div class="video-container">
            <video controls loop>
              <source src="static/videos/钳子.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-caption">复杂环境下钳子抓取</div>
        </div>
      </div>

      <!-- 第二行视频（动态环境） -->
      <div class="columns is-multiline is-centered" style="margin-top: 1rem;">
        <div class="column is-one-fifth video-item">
          <div class="video-container">
            <video controls loop>
              <source src="static/videos/扳手_动态.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-caption">动态环境下扳手抓取</div>
        </div>
        <div class="column is-one-fifth video-item">
          <div class="video-container">
            <video controls loop>
              <source src="static/videos/锤子_动态.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-caption">动态环境下锤子抓取</div>
        </div>
        <div class="column is-one-fifth video-item">
          <div class="video-container">
            <video controls loop>
              <source src="static/videos/搓刀_动态.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-caption">动态环境下搓刀抓取</div>
        </div>
        <div class="column is-one-fifth video-item">
          <div class="video-container">
            <video controls loop>
              <source src="static/videos/螺丝刀_动态.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-caption">动态环境下螺丝刀抓取</div>
        </div>
        <div class="column is-one-fifth video-item">
          <div class="video-container">
            <video controls loop>
              <source src="static/videos/钳子_动态.mp4" type="video/mp4">
            </video>
          </div>
          <div class="video-caption">动态环境下钳子抓取</div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
          <p>
            This means you are free to borrow the <a
              href="https://github.com/nerfies/nerfies.github.io">source code</a> of this website,
            we just ask that you link back to this page in the footer.
            Please remember to remove the analytics code included in the header of the website which
            you do not want on your website.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

  
</body>
</html>
